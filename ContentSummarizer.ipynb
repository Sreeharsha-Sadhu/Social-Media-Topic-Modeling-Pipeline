{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stage 1: Synthetic Social Graph & Persona Generation.",
   "id": "a2886ddfc6bc4bca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:04.079496Z",
     "start_time": "2025-11-03T16:34:04.070993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_USERS = 25\n",
    "PERSONAS = [\n",
    "    \"AI Researcher\",\n",
    "    \"Data Scientist\",\n",
    "    \"Web Developer\",\n",
    "    \"Financial Analyst\",\n",
    "    \"NBA Fanatic\",\n",
    "    \"Indie Gamer\",\n",
    "    \"Political Commentator\",\n",
    "    \"World Traveler\",\n",
    "    \"Aspiring Chef\"\n",
    "]\n",
    "AVG_FOLLOWS_PER_USER = 8\n",
    "\n",
    "# --- 1.1. & 1.2. Define Personas & Create Users ---\n",
    "\n",
    "def create_users(num_users, personas):\n",
    "    \"\"\"\n",
    "    Generates a list of user objects, each with a unique ID and\n",
    "    a primary and secondary persona.\n",
    "    \"\"\"\n",
    "    users = []\n",
    "    for i in range(1, num_users + 1):\n",
    "        primary_persona = random.choice(personas)\n",
    "        secondary_persona = random.choice([p for p in personas if p != primary_persona])\n",
    "\n",
    "        users.append({\n",
    "            \"user_id\": f\"user_{i}\",\n",
    "            \"username\": f\"user_{i}\",\n",
    "            \"personas\": [primary_persona, secondary_persona]\n",
    "        })\n",
    "    return users\n",
    "\n",
    "print(f\"Creating {NUM_USERS} users...\")\n",
    "users = create_users(NUM_USERS, PERSONAS)\n",
    "\n",
    "# --- 1.3. Build the Social Graph (NetworkX) ---\n",
    "\n",
    "def create_social_graph(users, avg_follows):\n",
    "    \"\"\"\n",
    "    Creates a NetworkX DiGraph (directed graph) to represent\n",
    "    \"follows\" relationships.\n",
    "\n",
    "    Users are more likely to follow others with shared personas.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all users as nodes to the graph\n",
    "    user_ids = [u[\"user_id\"] for u in users]\n",
    "    G.add_nodes_from(user_ids)\n",
    "\n",
    "    # Create \"follows\" (edges)\n",
    "    for user in users:\n",
    "        user_id = user[\"user_id\"]\n",
    "        user_personas = set(user[\"personas\"])\n",
    "\n",
    "        # Create a pool of potential users to follow (excluding self)\n",
    "        potential_follows = [u for u in users if u[\"user_id\"] != user_id]\n",
    "\n",
    "        # Weight the pool: 5x more likely to follow someone with a shared persona\n",
    "        weights = []\n",
    "        for pf in potential_follows:\n",
    "            pf_personas = set(pf[\"personas\"])\n",
    "            if user_personas.intersection(pf_personas):\n",
    "                weights.append(5)  # Higher weight for shared interest\n",
    "            else:\n",
    "                weights.append(1)  # Normal weight\n",
    "\n",
    "        # Decide how many users this person will follow\n",
    "        num_to_follow = random.randint(avg_follows - 3, avg_follows + 3)\n",
    "\n",
    "        # Select and add the edges\n",
    "        if potential_follows: # Ensure the list is not empty\n",
    "            followed_list = random.choices(potential_follows, weights=weights, k=num_to_follow)\n",
    "\n",
    "            # Add edges to the graph\n",
    "            for followed_user in followed_list:\n",
    "                G.add_edge(user_id, followed_user[\"user_id\"])\n",
    "\n",
    "    return G\n",
    "\n",
    "print(\"Building social graph...\")\n",
    "social_graph = create_social_graph(users, AVG_FOLLOWS_PER_USER)\n",
    "\n",
    "# --- Verification & Output ---\n",
    "\n",
    "print(\"\\n--- Stage 1 Complete ---\")\n",
    "print(f\"Total users created: {len(users)}\")\n",
    "print(f\"Example user:\\n{json.dumps(users[0], indent=2)}\")\n",
    "\n",
    "print(\"\\nSocial Graph Stats:\")\n",
    "print(f\"Total nodes (users): {social_graph.number_of_nodes()}\")\n",
    "print(f\"Total edges (follows): {social_graph.number_of_edges()}\")\n",
    "\n",
    "# Example: Check who user_1 follows\n",
    "user_1_follows = list(social_graph.successors(\"user_1\"))\n",
    "print(f\"\\nuser_1 follows {len(user_1_follows)} users: {user_1_follows[:5]}...\")\n",
    "\n",
    "# Example: Check who follows user_1\n",
    "user_1_followed_by = list(social_graph.predecessors(\"user_1\"))\n",
    "print(f\"user_1 is followed by {len(user_1_followed_by)} users: {user_1_followed_by[:5]}...\")\n",
    "\n",
    "# --- 1.4. Save Graph for later stages ---\n",
    "# We can save the user list and the graph (as an edge list)\n",
    "# to be used in the next stage.\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save users list\n",
    "with open(\"data/users.json\", \"w\") as f:\n",
    "    json.dump(users, f, indent=2)\n",
    "\n",
    "# Save the graph edges\n",
    "nx.write_edgelist(social_graph, \"data/follows.edgelist\")\n",
    "\n",
    "print(\"\\nUsers list saved to 'data/users.json'\")\n",
    "print(\"Social graph saved to 'data/follows.edgelist'\")"
   ],
   "id": "ae1928bafa168dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 25 users...\n",
      "Building social graph...\n",
      "\n",
      "--- Stage 1 Complete ---\n",
      "Total users created: 25\n",
      "Example user:\n",
      "{\n",
      "  \"user_id\": \"user_1\",\n",
      "  \"username\": \"user_1\",\n",
      "  \"personas\": [\n",
      "    \"Web Developer\",\n",
      "    \"NBA Fanatic\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Social Graph Stats:\n",
      "Total nodes (users): 25\n",
      "Total edges (follows): 160\n",
      "\n",
      "user_1 follows 6 users: ['user_24', 'user_21', 'user_6', 'user_8', 'user_7']...\n",
      "user_1 is followed by 4 users: ['user_3', 'user_18', 'user_23', 'user_25']...\n",
      "\n",
      "Users list saved to 'data/users.json'\n",
      "Social graph saved to 'data/follows.edgelist'\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stage 2: GenAI Content & Unique Feed Simulation",
   "id": "295e60310a1e54ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:04.136345Z",
     "start_time": "2025-11-03T16:34:04.102295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = \"data\"\n",
    "# Generate a variable number of posts per user\n",
    "\n",
    "AVG_POSTS_PER_USER = 40\n",
    "\n",
    "# --- 1. Define Post Templates (Simulating GenAI) ---\n",
    "# This dictionary mimics an LLM's ability to generate\n",
    "# persona-based content.\n",
    "\n",
    "POST_TEMPLATES = {\n",
    "    \"AI Researcher\": [\n",
    "        \"Just read a fascinating paper on {topic}. The implications for {field} are huge.\",\n",
    "        \"My hot take: {topic} is completely overhyped. The real breakthrough is still 5 years away.\",\n",
    "        \"Anyone else attending the {conf} conference? Excited to see the talks on {topic}.\",\n",
    "        \"Struggling with this new {library} implementation. Why is {detail} so non-intuitive?\",\n",
    "    ],\n",
    "    \"Data Scientist\": [\n",
    "        \"Finished my analysis on {dataset}. Turns out the primary driver for {metric} is {finding}.\",\n",
    "        \"Python's {library} is a lifesaver for {task}.\",\n",
    "        \"Building a new {model} model to predict {metric}. So far the results are... interesting.\",\n",
    "        \"Hot take: 90% of data science is just cleaning {dataset} data.\",\n",
    "    ],\n",
    "    \"Web Developer\": [\n",
    "        \"Why did we ever use {old_tech}? {new_tech} is so much cleaner.\",\n",
    "        \"Just deployed the new {feature} to prod. Fingers crossed!\",\n",
    "        \"TIL about this weird CSS bug in {browser}. Nightmare fuel.\",\n",
    "        \"Debating {framework_a} vs {framework_b} for the new project. Thoughts?\",\n",
    "    ],\n",
    "    \"Financial Analyst\": [\n",
    "        \"{stock} is looking seriously {sentiment} after their earnings call.\",\n",
    "        \"The {market_event} is going to have a major impact on {sector} stocks.\",\n",
    "        \"My model predicts a {movement} for {stock} in Q{quarter}.\",\n",
    "        \"Deep dive into {company}'s 10-K. Their {metric} looks suspicious.\",\n",
    "    ],\n",
    "    \"NBA Fanatic\": [\n",
    "        \"Can you believe that {player} trade? The {team} got fleeced!\",\n",
    "        \"{player} is the GOAT, I don't care what anyone says.\",\n",
    "        \"That game last night was insane. {team} totally choked in the {quarter}.\",\n",
    "        \"My prediction for the finals: {team} vs {team}.\",\n",
    "    ],\n",
    "    \"Indie Gamer\": [\n",
    "        \"Just sank 40 hours into {game}. It's a masterpiece of {genre}.\",\n",
    "        \"Stop playing {aaa_game} and go play {game}. You won't regret it.\",\n",
    "        \"The art style in {game} is just breathtaking.\",\n",
    "        \"Shoutout to the solo dev of {game}. Incredible achievement.\",\n",
    "    ],\n",
    "    \"Political Commentator\": [\n",
    "        \"The new {policy} is a disaster for {group}.\",\n",
    "        \"Can't believe what {politician} said about {issue}. Completely out of touch.\",\n",
    "        \"The upcoming {election} is the most important one yet.\",\n",
    "        \"Reading the latest poll on {issue}. The numbers are surprising.\",\n",
    "    ],\n",
    "    \"World Traveler\": [\n",
    "        \"Just landed in {city}! The {food} is incredible.\",\n",
    "        \"Back from {country}. My favorite part was definitely {activity}.\",\n",
    "        \"Packing for {country}. Any tips for {activity}?\",\n",
    "        \"That {airline} flight was rough, but the view of {landmark} was worth it.\",\n",
    "    ],\n",
    "    \"Aspiring Chef\": [\n",
    "        \"Tonight's experiment: {dish} with a {ingredient} twist. It actually worked!\",\n",
    "        \"Perfected my {technique} for {food}. The secret is {secret}.\",\n",
    "        \"I will never buy {food} from a store again. Homemade is so much better.\",\n",
    "        \"Failed attempt at {dish}. It was a {texture} mess.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- 2. Define Topic Fillers ---\n",
    "# These will be slotted into the templates above.\n",
    "\n",
    "TOPIC_FILLERS = {\n",
    "    \"{topic}\": [\"RAG systems\", \"scaling laws\", \"GANs\", \"customer churn\", \"React hooks\", \"CSS grid\", \"inflation\", \"the playoffs\", \"Stardew Valley\", \"immigration policy\", \"Japan\", \"sourdough bread\"],\n",
    "    \"{field}\": [\"NLP\", \"robotics\", \"e-commerce\", \"frontend dev\", \"macroeconomics\"],\n",
    "    \"{conf}\": [\"NeurIPS\", \"ICLR\", \"WWDC\", \"JSConf\"],\n",
    "    \"{library}\": [\"PyTorch\", \"Pandas\", \"React\", \"D3.js\"],\n",
    "    \"{detail}\": [\"the data loader\", \"the async handling\", \"the state management\"],\n",
    "    \"{dataset}\": [\"sales data\", \"user logs\", \"sensor data\"],\n",
    "    \"{metric}\": [\"conversion rate\", \"user engagement\", \"stock price\"],\n",
    "    \"{finding}\": [\"seasonal trends\", \"user location\", \"ad spend\"],\n",
    "    \"{task}\": [\"ETL\", \"data viz\", \"model training\"],\n",
    "    \"{model}\": [\"regression\", \"neural net\", \"prophet\"],\n",
    "    \"{old_tech}\": [\"jQuery\", \"AngularJS\", \"legacy PHP\"],\n",
    "    \"{new_tech}\": [\"Svelte\", \"Next.js\", \"FastAPI\"],\n",
    "    \"{feature}\": [\"checkout page\", \"auth flow\", \"dashboard\"],\n",
    "    \"{browser}\": [\"Safari\", \"Chrome\", \"Firefox\"],\n",
    "    \"{framework_a}\": [\"React\", \"Vue\"],\n",
    "    \"{framework_b}\": [\"Svelte\", \"Angular\"],\n",
    "    \"{stock}\": [\"TSLA\", \"AAPL\", \"GOOGL\", \"AMZN\"],\n",
    "    \"{sentiment}\": [\"undervalued\", \"overvalued\", \"volatile\"],\n",
    "    \"{market_event}\": [\"Fed rate hike\", \"CPI report\"],\n",
    "    \"{sector}\": [\"tech\", \"healthcare\", \"energy\"],\n",
    "    \"{movement}\": [\"10% upside\", \"20% drop\"],\n",
    "    \"{quarter}\": [\"1\", \"2\", \"3\", \"4\"],\n",
    "    \"{company}\": [\"Enron\", \"Meta\", \"startup X\"],\n",
    "    \"{player}\": [\"LeBron\", \"Jordan\", \"Curry\", \"Wemby\"],\n",
    "    \"{team}\": [\"Lakers\", \"Celtics\", \"Knicks\", \"Bulls\"],\n",
    "    \"{game}\": [\"Hades\", \"Hollow Knight\", \"Dave the Diver\"],\n",
    "    \"{genre}\": [\"metroidvania\", \"roguelike\", \"farming sim\"],\n",
    "    \"{aaa_game}\": [\"Call of Duty\", \"Assassin's Creed\"],\n",
    "    \"{policy}\": [\"tax bill\", \"healthcare reform\"],\n",
    "    \"{group}\": [\"small businesses\", \"students\"],\n",
    "    \"{politician}\": [\"the president\", \"senator X\"],\n",
    "    \"{issue}\": [\"climate change\", \"the economy\"],\n",
    "    \"{election}\": [\"midterm\", \"primary\"],\n",
    "    \"{city}\": [\"Tokyo\", \"Rome\", \"Bangkok\"],\n",
    "    \"{food}\": [\"ramen\", \"pasta\", \"pad thai\"],\n",
    "    \"{country}\": [\"Italy\", \"Thailand\", \"Argentina\"],\n",
    "    \"{activity}\": [\"hiking\", \"scuba diving\", \"visiting museums\"],\n",
    "    \"{airline}\": [\"Ryanair\", \"Spirit\"],\n",
    "    \"{landmark}\": [\"the Alps\", \"the coastline\"],\n",
    "    \"{dish}\": [\"beef bourguignon\", \"a souffle\", \"pho\"],\n",
    "    \"{ingredient}\": [\"cardamom\", \"truffle oil\"],\n",
    "    \"{technique}\": [\"sous-vide\", \"maillard reaction\"],\n",
    "    \"{secret}\": [\"more butter\", \"patience\"],\n",
    "    \"{texture}\": [\"soggy\", \"dense\"]\n",
    "}\n",
    "\n",
    "# --- 3. Helper Functions ---\n",
    "\n",
    "def get_random_timestamp():\n",
    "    \"\"\"Generates a random timestamp within the last 30 days.\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    delta = timedelta(days=random.randint(0, 30),\n",
    "                      hours=random.randint(0, 23),\n",
    "                      minutes=random.randint(0, 59))\n",
    "    return (now - delta).isoformat() + \"Z\"\n",
    "\n",
    "def generate_post_content(personas):\n",
    "    \"\"\"\n",
    "    Picks a persona (preferring the primary one) and\n",
    "    generates a post by filling in a template.\n",
    "    \"\"\"\n",
    "    # 70% chance to use primary persona, 30% for secondary\n",
    "    persona = random.choices(personas, weights=[0.7, 0.3], k=1)[0]\n",
    "\n",
    "    # Get a random template for that persona\n",
    "    template = random.choice(POST_TEMPLATES[persona])\n",
    "\n",
    "    # Fill in the blanks\n",
    "    content = template\n",
    "    # Find all placeholders in the template\n",
    "    placeholders = [key for key in TOPIC_FILLERS.keys() if key in content]\n",
    "\n",
    "    for key in placeholders:\n",
    "        content = content.replace(key, random.choice(TOPIC_FILLERS[key]), 1)\n",
    "\n",
    "    return content\n",
    "\n",
    "# --- 4. Main Script Execution ---\n",
    "\n",
    "print(\"--- Starting Stage 2 ---\")\n",
    "\n",
    "# Load users from Stage 1\n",
    "users_path = f\"{DATA_DIR}/users.json\"\n",
    "print(f\"Loading users from '{users_path}'...\")\n",
    "try:\n",
    "    with open(users_path, 'r') as f:\n",
    "        users = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: '{users_path}' not found. Please run Stage 1 first.\")\n",
    "    exit() # Or handle error appropriately in a notebook\n",
    "\n",
    "# Generate \"Original Posts\" (Simulated LLM)\n",
    "all_posts = []\n",
    "print(f\"Generating synthetic posts for {len(users)} users...\")\n",
    "\n",
    "for user in tqdm(users, desc=\"Generating Posts\"):\n",
    "    user_id = user['user_id']\n",
    "    personas = user['personas']\n",
    "\n",
    "    # Each user gets a slightly different number of posts\n",
    "    num_posts = random.randint(AVG_POSTS_PER_USER - 15, AVG_POSTS_PER_USER + 15)\n",
    "\n",
    "    for _ in range(num_posts):\n",
    "        post_content = generate_post_content(personas)\n",
    "\n",
    "        all_posts.append({\n",
    "            \"post_id\": str(uuid.uuid4()),\n",
    "            \"author_id\": user_id,\n",
    "            \"content\": post_content,\n",
    "            \"created_at\": get_random_timestamp()\n",
    "        })\n",
    "\n",
    "# Save Posts to JSON\n",
    "posts_path = f\"{DATA_DIR}/posts.json\"\n",
    "print(f\"\\nSaving {len(all_posts)} posts to '{posts_path}'...\")\n",
    "with open(posts_path, \"w\") as f:\n",
    "    json.dump(all_posts, f, indent=2)\n",
    "\n",
    "# Convert 'follows.edgelist' to 'follows.json' for easier Spark ingestion\n",
    "edgelist_path = f\"{DATA_DIR}/follows.edgelist\"\n",
    "follows_json_path = f\"{DATA_DIR}/follows.json\"\n",
    "\n",
    "print(f\"Converting '{edgelist_path}' to '{follows_json_path}'...\")\n",
    "try:\n",
    "    G = nx.read_edgelist(edgelist_path, create_using=nx.DiGraph())\n",
    "    follows_list = []\n",
    "    for follower, followed in G.edges():\n",
    "        follows_list.append({\n",
    "            \"follower_id\": follower,\n",
    "            \"followed_id\": followed\n",
    "        })\n",
    "\n",
    "    with open(follows_json_path, \"w\") as f:\n",
    "        json.dump(follows_list, f, indent=2)\n",
    "    print(f\"Successfully converted {len(follows_list)} follow relationships.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: '{edgelist_path}' not found. Did Stage 1 run correctly?\")\n",
    "\n",
    "print(\"\\n--- Stage 2 Complete ---\")\n",
    "print(f\"Raw data landing zone '{DATA_DIR}' now contains:\")\n",
    "print(f\"- users.json\")\n",
    "print(f\"- posts.json\")\n",
    "print(f\"- follows.json\")"
   ],
   "id": "94fca5693e816232",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stage 2 ---\n",
      "Loading users from 'data/users.json'...\n",
      "Generating synthetic posts for 25 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Posts:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\sreeh\\AppData\\Local\\Temp\\ipykernel_17368\\3586460790.py:130: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()\n",
      "Generating Posts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 2773.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 1024 posts to 'data/posts.json'...\n",
      "Converting 'data/follows.edgelist' to 'data/follows.json'...\n",
      "Successfully converted 160 follow relationships.\n",
      "\n",
      "--- Stage 2 Complete ---\n",
      "Raw data landing zone 'data' now contains:\n",
      "- users.json\n",
      "- posts.json\n",
      "- follows.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stage 3: PySpark ETL & PostgreSQL Ingestion",
   "id": "1bad059f386087af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:04.145516Z",
     "start_time": "2025-11-03T16:34:04.142638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DB_NAME = \"mmds\"\n",
    "DB_USER = \"sadhu\"\n",
    "DB_PASS = \"12345678\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\""
   ],
   "id": "fe4d44daf23e714a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:04.198508Z",
     "start_time": "2025-11-03T16:34:04.150529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "conn_string = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASS}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "\n",
    "# SQL statements to create our schema\n",
    "create_tables_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id TEXT PRIMARY KEY,\n",
    "    username TEXT,\n",
    "    personas TEXT[]\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS follows (\n",
    "    follower_id TEXT REFERENCES users(user_id),\n",
    "    followed_id TEXT REFERENCES users(user_id),\n",
    "    PRIMARY KEY (follower_id, followed_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    post_id TEXT PRIMARY KEY,\n",
    "    author_id TEXT REFERENCES users(user_id),\n",
    "    content TEXT,\n",
    "    cleaned_content TEXT,\n",
    "    created_at TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(create_tables_sql)\n",
    "            conn.commit()\n",
    "    print(\"--- Stage 3.A: Setup Complete ---\")\n",
    "    print(\"Successfully connected and created tables (if they didn't exist).\")\n",
    "\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(\"\\n--- ðŸš¨ ERROR: Could not connect to PostgreSQL ---\")\n",
    "    print(f\"Error details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ],
   "id": "46a9faad3b37451e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 3.A: Setup Complete ---\n",
      "Successfully connected and created tables (if they didn't exist).\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:04.251585Z",
     "start_time": "2025-11-03T16:34:04.203540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psycopg2\n",
    "\n",
    "conn_string = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASS}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "\n",
    "# TRUNCATE tables in the correct order\n",
    "truncate_sql = \"\"\"\n",
    "TRUNCATE TABLE posts, follows;\n",
    "TRUNCATE TABLE users RESTART IDENTITY CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Starting Stage 3.B.1: Pre-ETL Table Truncation ---\")\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(truncate_sql)\n",
    "            conn.commit()\n",
    "    print(\"Successfully truncated tables: 'posts', 'follows', and 'users'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- ðŸš¨ ERROR: Could not truncate tables ---\")\n",
    "    print(f\"Error details: {e}\")"
   ],
   "id": "e7b7a838d0644a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stage 3.B.1: Pre-ETL Table Truncation ---\n",
      "Successfully truncated tables: 'posts', 'follows', and 'users'.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:07.169538Z",
     "start_time": "2025-11-03T16:34:04.257589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "print(\"\\n--- Starting Stage 3.B.2: PySpark ETL (SQL-Native Functions Only) ---\")\n",
    "\n",
    "# --- 1. Initialize SparkSession ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SocialMediaETL\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession initialized.\")\n",
    "\n",
    "# --- 2. JDBC Config ---\n",
    "jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "properties = {\"user\": DB_USER, \"password\": DB_PASS, \"driver\": \"org.postgresql.Driver\"}\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# --- 3. Run ETL ---\n",
    "try:\n",
    "    # --- USERS ---\n",
    "    print(\"Processing 'users' data...\")\n",
    "    users_df = spark.read.option(\"multiline\", \"true\").json(f\"{DATA_DIR}/users.json\")\n",
    "    users_df.write.jdbc(jdbc_url, \"users\", mode=\"append\", properties=properties)\n",
    "    print(f\"Successfully loaded {users_df.count()} users into 'users' table.\")\n",
    "\n",
    "    # --- FOLLOWS ---\n",
    "    print(\"Processing 'follows' data...\")\n",
    "    follows_df = spark.read.option(\"multiline\", \"true\").json(f\"{DATA_DIR}/follows.json\")\n",
    "    follows_df.write.jdbc(jdbc_url, \"follows\", mode=\"append\", properties=properties)\n",
    "    print(f\"Successfully loaded {follows_df.count()} relationships into 'follows' table.\")\n",
    "\n",
    "    # --- POSTS ---\n",
    "    print(\"Processing 'posts' data (SQL-native functions)...\")\n",
    "    posts_df = spark.read.option(\"multiline\", \"true\").json(f\"{DATA_DIR}/posts.json\")\n",
    "\n",
    "    # --- This is the new, 100% stable cleaning logic ---\n",
    "    # It uses only Spark's built-in functions. No Python worker is needed.\n",
    "    cleaned_col = F.lower(F.col(\"content\"))\n",
    "    cleaned_col = F.regexp_replace(cleaned_col, \"http\\\\S+\", \"\")  # Remove URLs\n",
    "    cleaned_col = F.regexp_replace(cleaned_col, \"[^a-z0-9\\\\s]\", \"\") # Remove non-alpha\n",
    "    cleaned_col = F.regexp_replace(cleaned_col, \"\\\\s+\", \" \")      # Collapse whitespace\n",
    "    cleaned_col = F.trim(cleaned_col)\n",
    "    cleaned_col = F.when(cleaned_col == \"\", None).otherwise(cleaned_col)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    posts_df_final = posts_df.withColumn(\"cleaned_content\", cleaned_col) \\\n",
    "                              .withColumn(\"created_at\", F.to_timestamp(F.col(\"created_at\"))) \\\n",
    "                              .select(\"post_id\", \"author_id\", \"content\", \"cleaned_content\", \"created_at\")\n",
    "\n",
    "    posts_df_final.write.jdbc(jdbc_url, \"posts\", mode=\"append\", properties=properties)\n",
    "    print(f\"Successfully loaded {posts_df_final.count()} posts into 'posts' table.\")\n",
    "\n",
    "    print(\"\\n--- PySpark ETL Job Complete! ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ðŸš¨ ERROR during Spark ETL ---\")\n",
    "    print(f\"Error details: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()\n",
    "    print(\"SparkSession stopped.\")"
   ],
   "id": "c3c520bfdb8c9c2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Stage 3.B.2: PySpark ETL (SQL-Native Functions Only) ---\n",
      "SparkSession initialized.\n",
      "Processing 'users' data...\n",
      "Successfully loaded 25 users into 'users' table.\n",
      "Processing 'follows' data...\n",
      "Successfully loaded 160 relationships into 'follows' table.\n",
      "Processing 'posts' data (SQL-native functions)...\n",
      "Successfully loaded 1024 posts into 'posts' table.\n",
      "\n",
      "--- PySpark ETL Job Complete! ---\n",
      "SparkSession stopped.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stage 4: Backend AI Core (Analysis Pipeline)",
   "id": "671713e0c1a4f8bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:07.302364Z",
     "start_time": "2025-11-03T16:34:07.257023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psycopg2\n",
    "# ------------------------------\n",
    "\n",
    "conn_string = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASS}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "\n",
    "# SQL statements to create our new results tables\n",
    "create_results_tables_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_topics (\n",
    "    topic_id SERIAL PRIMARY KEY,\n",
    "    user_id TEXT REFERENCES users(user_id),\n",
    "    topic_name TEXT,\n",
    "    summary TEXT,\n",
    "    post_count INTEGER,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS post_topic_mapping (\n",
    "    post_id TEXT REFERENCES posts(post_id),\n",
    "    topic_id INTEGER REFERENCES user_topics(topic_id),\n",
    "    PRIMARY KEY (post_id, topic_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(create_results_tables_sql)\n",
    "            conn.commit()\n",
    "    print(\"--- Stage 4.B: Setup Complete ---\")\n",
    "    print(\"Successfully created 'user_topics' and 'post_topic_mapping' tables.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- ðŸš¨ ERROR: Could not create results tables ---\")\n",
    "    print(f\"Error details: {e}\")"
   ],
   "id": "8783515401f7a3d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 4.B: Setup Complete ---\n",
      "Successfully created 'user_topics' and 'post_topic_mapping' tables.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:34:31.346930Z",
     "start_time": "2025-11-03T16:34:07.307370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline\n",
    "from psycopg2.extras import execute_values\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- 1. Database Connector Module ---\n",
    "def get_db_connection():\n",
    "    \"\"\"Establishes a new connection to the PostgreSQL database.\"\"\"\n",
    "    conn_string = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASS}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "    return psycopg2.connect(conn_string)\n",
    "\n",
    "def clear_previous_analysis(conn, user_id):\n",
    "    \"\"\"\n",
    "    Clears out old analysis results for a user to prevent duplicates.\n",
    "    Uses ON DELETE CASCADE from the foreign keys.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # This is the \"parent\" table. Deleting from here will cascade\n",
    "        # and delete all mappings from 'post_topic_mapping'.\n",
    "        cur.execute(\"DELETE FROM user_topics WHERE user_id = %s\", (user_id,))\n",
    "        conn.commit()\n",
    "    print(f\"Cleared old analysis results for {user_id}.\")\n",
    "\n",
    "# --- 2. Feed Retrieval Logic ---\n",
    "def get_feed_for_user(conn, user_id):\n",
    "    \"\"\"\n",
    "    Fetches the unique feed for a given user.\n",
    "    Returns a DataFrame of post_ids and their cleaned_content.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching feed for {user_id}...\")\n",
    "\n",
    "    # This query finds all posts written by people the user_id follows\n",
    "    sql_query = \"\"\"\n",
    "    SELECT p.post_id, p.cleaned_content\n",
    "    FROM posts p\n",
    "    JOIN follows f ON p.author_id = f.followed_id\n",
    "    WHERE f.follower_id = %s\n",
    "      AND p.cleaned_content IS NOT NULL\n",
    "      AND p.cleaned_content != '';\n",
    "    \"\"\"\n",
    "\n",
    "    # Use pandas to read SQL results directly into a DataFrame\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql_query, (user_id,))\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['post_id', 'cleaned_content'])\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No posts found for {user_id}'s feed.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Found {len(df)} posts in {user_id}'s feed.\")\n",
    "    return df\n",
    "\n",
    "# --- 3. AI Core (Topic Modeling & Summarization) ---\n",
    "def run_analysis_pipeline(user_id):\n",
    "    \"\"\"\n",
    "    Main function to run the full AI analysis pipeline for a user.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "\n",
    "        # Step 0: Clear old data\n",
    "        clear_previous_analysis(conn, user_id)\n",
    "\n",
    "        # Step 1: Get the user's feed\n",
    "        feed_df = get_feed_for_user(conn, user_id)\n",
    "        if feed_df is None or len(feed_df) < 10: # Min posts for BERTopic\n",
    "            print(f\"Not enough posts ({len(feed_df)}) to analyze. Aborting.\")\n",
    "            return\n",
    "\n",
    "        posts_list = feed_df['cleaned_content'].tolist()\n",
    "        post_id_list = feed_df['post_id'].tolist()\n",
    "\n",
    "        # Step 2: Initialize AI Models\n",
    "        # BERTopic: Use a simpler model for speed in our notebook\n",
    "        print(\"Initializing BERTopic model...\")\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=\"all-MiniLM-L6-v2\", # Fast & small\n",
    "            min_topic_size=5,                     # Find topics with at least 5 posts\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Summarizer: Use a small, fast model\n",
    "        print(\"Initializing Summarization model (this may take a moment)...\")\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"sshleifer/distilbart-cnn-6-6\", # Small & fast\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Step 3: Run Topic Modeling\n",
    "        print(\"Running BERTopic.This may take a few minutes...\")\n",
    "        topics, _ = topic_model.fit_transform(posts_list)\n",
    "\n",
    "        # Add topic assignments back to our DataFrame\n",
    "        feed_df['topic'] = topics\n",
    "\n",
    "        # Get topic info (names, post counts)\n",
    "        # We'll analyze topics -1 (outliers), 0, 1, 2...\n",
    "        topic_info_df = topic_model.get_topic_info()\n",
    "        print(f\"BERTopic found {len(topic_info_df) - 1} topics.\")\n",
    "\n",
    "        # --- Step 4: Summarize and Save Each Topic ---\n",
    "        print(\"Summarizing topics and saving results...\")\n",
    "\n",
    "        # We'll collect all data to insert in batches\n",
    "        all_post_topic_mappings = []\n",
    "\n",
    "        # Use tqdm for a nice progress bar\n",
    "        for _, topic_row in tqdm(topic_info_df.iterrows(), total=topic_info_df.shape[0]):\n",
    "            topic_num = topic_row['Topic']\n",
    "            topic_name = topic_row['Name']\n",
    "\n",
    "            # Skip the outlier topic (-1)\n",
    "            if topic_num == -1:\n",
    "                continue\n",
    "\n",
    "            # Get all posts for this topic\n",
    "            topic_posts_df = feed_df[feed_df['topic'] == topic_num]\n",
    "            topic_posts = topic_posts_df['cleaned_content'].tolist()\n",
    "            post_count = len(topic_posts)\n",
    "\n",
    "            # Combine all posts into one document for the summarizer\n",
    "            doc_to_summarize = \" . \".join(topic_posts)\n",
    "\n",
    "            # Ensure doc is not too long for the model (e.g., ~1024 tokens)\n",
    "            # A simple way is to cap by character length\n",
    "            max_chars = 4000\n",
    "            if len(doc_to_summarize) > max_chars:\n",
    "                doc_to_summarize = doc_to_summarize[:max_chars]\n",
    "\n",
    "            # Run summarization\n",
    "            summary_result = summarizer(doc_to_summarize, min_length=15, max_length=50)\n",
    "            summary_text = summary_result[0]['summary_text']\n",
    "\n",
    "            # Save topic to DB and get its new primary key (topic_id)\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO user_topics (user_id, topic_name, summary, post_count)\n",
    "                    VALUES (%s, %s, %s, %s)\n",
    "                    RETURNING topic_id;\n",
    "                    \"\"\",\n",
    "                    (user_id, topic_name, summary_text, post_count)\n",
    "                )\n",
    "                new_topic_id = cur.fetchone()[0]\n",
    "                conn.commit()\n",
    "\n",
    "            # Collect the post_id <-> topic_id mappings\n",
    "            for post_id in topic_posts_df['post_id'].tolist():\n",
    "                all_post_topic_mappings.append((post_id, new_topic_id))\n",
    "\n",
    "        # Step 5: Batch-insert all post-topic mappings\n",
    "        print(f\"Saving {len(all_post_topic_mappings)} post-topic mappings...\")\n",
    "        with conn.cursor() as cur:\n",
    "            execute_values(\n",
    "                cur,\n",
    "                \"INSERT INTO post_topic_mapping (post_id, topic_id) VALUES %s\",\n",
    "                all_post_topic_mappings\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "        print(\"\\n--- Stage 4: Analysis Complete! ---\")\n",
    "        print(f\"Successfully analyzed and saved results for {user_id}.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"--- ðŸš¨ ERROR in Stage 4 ---\")\n",
    "        print(f\"Error details: {error}\")\n",
    "        if conn:\n",
    "            conn.rollback() # Roll back any failed transactions\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# --- 4.D: Example Usage ---\n",
    "# Pick a user_id to analyze. 'user_1' is a good test.\n",
    "USER_TO_ANALYZE = 'user_1'\n",
    "run_analysis_pipeline(USER_TO_ANALYZE)"
   ],
   "id": "5dd25cd7a02b9381",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared old analysis results for user_1.\n",
      "Fetching feed for user_1...\n",
      "Found 257 posts in user_1's feed.\n",
      "Initializing BERTopic model...\n",
      "Initializing Summarization model (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic.This may take a few minutes...\n",
      "BERTopic found 23 topics.\n",
      "Summarizing topics and saving results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11/24 [00:04<00:04,  2.84it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:08<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 254 post-topic mappings...\n",
      "\n",
      "--- Stage 4: Analysis Complete! ---\n",
      "Successfully analyzed and saved results for user_1.\n",
      "Database connection closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:35:55.443547Z",
     "start_time": "2025-11-03T16:35:55.437423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psycopg2\n",
    "from IPython.display import clear_output, display\n",
    "import pandas as pd\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Establishes a new connection to the PostgreSQL database.\"\"\"\n",
    "    conn_string = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASS}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "    return psycopg2.connect(conn_string)\n",
    "\n",
    "def list_all_users():\n",
    "    \"\"\"Fetches and displays all users from the database.\"\"\"\n",
    "    print(\"--- ðŸ‘¤ All Users ---\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        # Use pandas to pretty-print the user list\n",
    "        df = pd.read_sql(\"SELECT user_id, username, personas FROM users ORDER BY user_id\", conn)\n",
    "        display(df) # Use display() for nice notebook formatting\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ Error listing users: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def view_results_for_user(user_id):\n",
    "    \"\"\"Fetches and displays the last analysis results for a user.\"\"\"\n",
    "    print(f\"\\n--- ðŸ“ˆ Viewing Analysis for: {user_id} ---\")\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT topic_name, summary, post_count\n",
    "                FROM user_topics\n",
    "                WHERE user_id = %s\n",
    "                ORDER BY post_count DESC\n",
    "                \"\"\",\n",
    "                (user_id,)\n",
    "            )\n",
    "            results = cur.fetchall()\n",
    "\n",
    "            if not results:\n",
    "                print(f\"No results found for {user_id}.\")\n",
    "                print(\"Please run an analysis (Option 2) for this user first.\")\n",
    "                return\n",
    "\n",
    "            for i, (topic_name, summary, post_count) in enumerate(results):\n",
    "                print(\"\\n\" + \"=\" * 40)\n",
    "                print(f\"   TOPIC {i}: {topic_name.split('_', 1)[1]}\")\n",
    "                print(f\"   POSTS: {post_count}\")\n",
    "                print(f\"   SUMMARY: {summary}\")\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ Error viewing results: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "print(\"Stage 5.A: UI Helper functions defined.\")"
   ],
   "id": "9052b12e8485ab01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 5.A: UI Helper functions defined.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This cell runs the main application loop.\n",
    "# It requires the functions from Stage 4.C and Stage 5.A to be in memory.\n",
    "\n",
    "def main_menu():\n",
    "    \"\"\"The main interactive console loop.\"\"\"\n",
    "    while True:\n",
    "        clear_output(wait=True) # Clears the output for a clean menu\n",
    "        print(\"=\" * 50)\n",
    "        print(\"  ðŸ“Š Social Media Topic & Summary Tool ðŸ“Š\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. List All Users\")\n",
    "        print(\"2. Run New Analysis for a User\")\n",
    "        print(\"3. View Last Analysis for a User\")\n",
    "        print(\"4. Exit\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        choice = input(\"Enter your choice (1-4): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            list_all_users()\n",
    "            input(\"\\nPress Enter to return to the menu...\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            user_id = input(\"Enter User ID to analyze (e.g., user_1): \")\n",
    "            try:\n",
    "                # This calls the function from your Stage 4 cell\n",
    "                print(f\"Starting new analysis for {user_id}...\")\n",
    "                run_analysis_pipeline(user_id)\n",
    "                print(f\"\\nâœ… Analysis complete for {user_id}!\")\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸš¨ An error occurred during analysis: {e}\")\n",
    "            input(\"\\nPress Enter to return to the menu...\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            user_id = input(\"Enter User ID to view (e.g., user_1): \")\n",
    "            view_results_for_user(user_id)\n",
    "            input(\"\\nPress Enter to return to the menu...\")\n",
    "\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            input(\"Invalid choice. Press Enter to try again...\")\n",
    "\n",
    "# --- Run the application ---\n",
    "main_menu()"
   ],
   "id": "4f1220511ddf365c",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 47\u001B[0m\n\u001B[0;32m     44\u001B[0m             \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid choice. Press Enter to try again...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# --- Run the application ---\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m \u001B[43mmain_menu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[19], line 17\u001B[0m, in \u001B[0;36mmain_menu\u001B[1;34m()\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4. Exit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m choice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEnter your choice (1-4): \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m choice \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     20\u001B[0m     list_all_users()\n",
      "File \u001B[1;32mF:\\venvs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\venvs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
